<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>DreamGrasp</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">

          <!-- Title. -->
          <h1 class="title is-2 publication-title">
            DreamGrasp: <br>
            <span class="subtitle is-3"> <font color="#363636">Zero-Shot 3D Multi-Object Reconstruction from <br>Partial-View Images for Robotic Manipulation</font></span>
          </h1>

          <!-- Authors. -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Young Hun Kim<sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://seungyeon-k.github.io">Seungyeon Kim</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.gabe-yhlee.com">Yonghyeon Lee</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/robotics.snu.ac.kr/fcp/">Frank C. Park</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Seoul National University,</span>
            <span class="author-block"><sup>2</sup>Massachusetts Institute of Technology</span>
          </div>

          <!-- Icons. -->
          <div class="column has-text-centered">
            <div class="publication-links">

              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2507.05627"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (arXiv)</span>
                </a>
              </span>

              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="."
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video (Coming soon)</span>
                </a>
              </span> -->

              <!-- Code Link. -->
              <span class="link-block">
                <a href="."
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span>

              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="."
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Dataset (Coming soon)</span>
                  </a>
              </span> -->

            </div>
          </div>

          <!-- Blank. -->
          <div class="columns is-centered">
            <div class="content">
              <h2 class="title is-3"></h2>
            </div>            
          </div>

          <!-- TL;DR. -->
          <div class="columns is-centered has-text-justified interpolation-panel">
            <div class="column is-full-width">
              <h2 class="title is-5">
                <font color="#808080">
                  <p></p><p></p><p></p>
                  TL;DR: This paper propose a zero-shot framework for 3D reconstruction and instance recognition from a 
                  few partial RGB views by leveraging pre-trained image generative models.
                </font>
              </h2>
            </div>
          </div>
          
          <!-- Blank. -->
          <div class="columns is-centered">
            <div class="content">
              <h2 class="title is-3"></h2>
            </div>            
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Partial-view 3D recognition -- reconstructing 3D geometry and identifying object instances from a few sparse RGB images -- is an exceptionally challenging yet practically essential task, particularly in cluttered, occluded real-world settings where full-view or reliable depth data are often unavailable. 
            Existing methods, whether based on strong symmetry priors or supervised learning on curated datasets, fail to generalize to such scenarios. 
            In this work, we introduce DreamGrasp, a framework that leverages the imagination capability of large-scale pre-trained image generative models to infer the unobserved parts of a scene. 
            By combining coarse 3D reconstruction, instance segmentation via contrastive learning, and text-guided instance-wise refinement, DreamGrasp circumvents limitations of prior methods and enables robust 3D reconstruction  in complex, multi-object environments. Our experiments show that DreamGrasp not only recovers accurate object geometry but also supports downstream tasks like sequential decluttering and target retrieval with high success rates.
          </p>
        </div>
      </div>
    </div>

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <!-- <div class="publication-video">
          <iframe src="https://www.youtube.com"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div> -->
        <div class="content has-text-centered">
          <p>

            Coming Soon...

          </p>
        </div>        
      </div>
    </div>

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
        <div class="content has-text-centered">
          <p>
            Coming soon
          </p>
        </div>        
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>

  
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Dataset. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Overview of DreamGrasp</h2>
        
        <h3 class="title is-4">Overall pipeline of DreamGrasp</h3>
        <div class="content has-text-justified">
          <b>(Observation)</b> Our method uses only two partial-view
          RGB images as input. <b>(Input process)</b> Instance masks and text prompts are extracted from the
          RGB images using SAM and ChatGPT, respectively. <b>(Coarse stage)</b> These inputs are
          used for initial scene-level geometry reconstruction, leveraging RGB and instance mask images
          with novel-view supervision guided by Zero123. <b>(Refinement stage)</b> The coarse scene is segmented
          using learned features, and each object is refined through instance-wise RGB input and novel-view
          supervision guided by a text-conditioned diffusion model.

        </div>

        <div class="columns">
          <div class="column has-text-centered">
            <img src="./assets/images/pipeline.jpg"
                 alt="core components"/>
          </div>
        </div>

        <h3 class="title is-4">Recognition Results</h3>
        <div class="content has-text-justified">
          DreamGrasp consistently outperforms all baselines across
          varying object counts. While CGC struggles with geometry prediction under partial views, Zero123*
          performs competitively at the scene level. However, its performance drops significantly in instance-
          wise geometry prediction, emphasizing the importance of the refinement stage
        </div>

        <div class="columns">
          <div class="column has-text-centered">
            <img src="./assets/images/recog_results3.png"
                 alt="core components"/>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!--  -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Object Manipulation with DreamGrasp</h2>

        <!--  -->
        <h3 class="title is-4">Sequential Declutter</h3>
        <div class="content has-text-justified">
          <b>Sequential Declutter</b> is the task of sequentially removing all objects in a scene through grasping.  
          For reconstruction methods that do not provide instance-wise segmentation, it is often necessary to re-run the 
          reconstruction process after each grasp, requiring reconstruction to be performed as many times as there are objects in the scene.  
          In contrast, thanks to the instance-wise geometry provided by DreamGrasp, a single reconstruction suffices. 
          This enables the entire decluttering sequence to be planned upfront from a single initial recognition step.
        </div>
        <div class="columns">
          <div class="column has-text-centered">
            <img src="./assets/images/real_clear_clutter_large2.png"
                 alt="core components"/>
          </div>
        </div>

        <!--  -->
        <h3 class="title is-4">Target Retrieval</h3>
        <div class="content has-text-justified">
          <p>
            <b>Target Retrieval</b> involves extracting a specific target object that is initially ungraspable due to occlusions by surrounding objects. 
            This requires rearranging obstacles within the workspace to make the target graspable. Prior works have proposed solving this task using 
            instance-wise 3D recognition, which requires an instance-wise grasp pose sampler, a grasp pose collision detector, and a model of rearrangement 
            dynamics (i.e., predicting how the scene changes after rearrangement). The first two components are provided by DreamGrasp. For rearrangement, 
            we adopt a simple pick-and-place formulation, where dynamics are approximated by moving the selected object from its current pose to a 
            predefined placement pose. Such rearrangement dynamics would not be accessible without instance-level identification of objects.
          </p>
        </div>
        <div class="columns">
          <div class="column has-text-centered">
            <img src="./assets/images/real_target_retrieval4.png"
                 alt="core components"/>
          </div>
        </div>

        <!-- Object Manipulation Results. -->
        <h3 class="title is-4">Object Manipulation Results</h3>
        <div class="content has-text-justified">
          <p>
            We demonstrate that DreamGrasp provides sufficiently accurate recognition results for downstream use by executing 
            two manipulation tasks in a cluttered shelf environment. We place various real-world objects including non-symmetric 
            objects and transparent objects on a shelf and perform object recognition.
          </p>
        </div>

        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/BfCdhoNRJz0?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>

      </div>
    </div>
  </div>
</section>


<!-- Citation. -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Citation</h2>
    <pre><code>
      @article{kim2025dreamgrasp,
        title={DreamGrasp: Zero-Shot 3D Multi-Object Reconstruction from Partial-View Images for Robotic Manipulation},
        author={Kim, Young Hun and Kim, Seungyeon and Lee, Yonghyeon and Park, Frank Chongwoo},
        journal={arXiv preprint arXiv:2507.05627},
        year={2025}
      }
    </code></pre>
  </div>
</section>

<!-- Footer. -->
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>
        Website template modified from <a href="https://nerfies.github.io/">NeRFies</a>.
      </p>
    </div>
  </div>
</footer>

</body>
</html>
